server_bin: /home/dev/workspace/llama.cpp/build/bin/llama-server
manager_port: 8080
restart_delay: 5s
max_restarts: 10
health_check_interval: 30s

# GPU backend: vulkan, cuda, rocm, rocm_rocr
gpu_backend: vulkan

# Default server arguments (applied to all instances)
host: "0.0.0.0"
ngl: 99
main_gpu: 0
context_length: 16384
cache_type_k: q8_0
cache_type_v: q8_0

instances:
  - name: dolphin-gpu0
    model: "bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF:IQ4_XS"
    port: 9090
    gpu_id: 0

  - name: dolphin-gpu1
    model: "bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF:IQ4_XS"
    port: 9091
    gpu_id: 1

  - name: dolphin-gpu2
    model: "bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF:IQ4_XS"
    port: 9092
    gpu_id: 2

  - name: dolphin-gpu3
    model: "bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF:IQ4_XS"
    port: 9093
    gpu_id: 3

  - name: dolphin-gpu4
    model: "bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF:IQ4_XS"
    port: 9094
    gpu_id: 4
